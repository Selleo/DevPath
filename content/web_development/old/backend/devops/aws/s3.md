# S3

Amazon Simple Storage Service is storage for the internet. You can use Amazon S3 to store and retrieve any amount of data at any time, from anywhere on the web. It can be easily used to host static sites.

- Key-value based object storage with unlimited storage, unlimited objects up to 5 TB for the internet
- is an **Object level storage** (not a Block level storage) and cannot be used to host OS or dynamic websites (but can work with Javascript SDK)
- provides **durability by redundantly storing objects on multiple facilities within a region**
- support **SSL encryption of data in transit** and **data encryption at rest**
- regularly **verifies the integrity** of data using checksums and provides auto healing capability
- integrates with CloudTrail, CloudWatch and SNS for event notifications
- **S3 resources**
  - consists of **bucket and objects** stored in the bucket which can be retrieved via a unique, developer-assigned key
  - **bucket names are globally unique**
  - **data model is a flat structure** with no hierarchies or folders
  - **Logical hierarchy** can be inferred using the keyname prefix e.g. Folder1/Object1
- **Bucket & Object Operations**
  - allows **retrieval of 1000 objects and provides pagination support** and is **NOT** suited for list or prefix queries with large number of objects
  - with a single put operations, 5GB size object can be uploaded
  - use **Multipart upload** to upload large objects up to 5 TB and is recommended for object size of over 100MB for fault tolerant uploads
  - support **Range HTTP Header** to retrieve partial objects for fault tolerant downloads where the network connectivity is poor
  - **Pre-Signed URLs** can also be used shared for uploading/downloading objects for **limited time without requiring AWS security credentials**
  - allows deletion of a single object or multiple objects (max 1000) in a single call
- **Multipart Uploads** allows
  - **parallel uploads** with improved throughput and bandwidth utilization
  - **fault tolerance and quick recovery** from network issues
  - ability to **pause and resume** uploads
  - **begin an upload before the final object size is known**
- **Versioning**
  - allows preserve, retrieve, and restore every version of every object
  - **protects individual files** but does **NOT protect from Bucket deletion**
- **Storage tiers**
  - Standard
    - default storage class
    - **99.999999999% durability** & **99.99% availability**
    - Low latency and high throughput performance
    - designed to **sustain the loss of data in a two** facilities
  - Standard IA
    - optimized for **long-lived and less frequently** accessed data
    - designed to **sustain the loss of data in a two** facilities
    - **99.999999999% durability** & **99.9% availability**
    - suitable for **objects greater than 128 KB** kept for at **least 30 days**
  - Reduced Redundancy Storage
    - designed for **noncritical, reproducible data** stored at lower levels of redundancy than the STANDARD storage class
    - **reduces storage costs**
    - **99.99% durability** & **99.99% availability**
    - designed to **sustain the loss of data in a single** facility
  - [Glacier](https://aws.amazon.com/glacier/)
    - suitable for **archiving data** where **data access is infrequent and retrieval time of several (3-5) hours is acceptable**
    - **99.999999999% durability**
- allows **Lifecycle Management policies**
  - **transition** to move objects to different storage classes and Glacier
  - **expiration** to remove objects
- **Data Consistency Model**
  - provide **read-after-write consistency for PUTS of new objects** and **eventual consistency for overwrite PUTS and DELETES**
  - for new objects, **synchronously stores data across multiple facilities** before returning success
  - **updates to a single key are atomic**
- **Security**
  - **IAM policies** - grant users within your own AWS account permission to access S3 resources
  - **Bucket and Object ACL** - grant other AWS accounts (not specific users) access to S3 resources
  - **Bucket policies** - allows to add or deny permissions across some or all of the objects within a single bucket
- **Best Practices**
  - **use random hash prefix for keys and ensure a random access pattern**, as S3 stores object lexicographically randomness helps distribute the contents across multiple partitions for better performance
  - use parallel threads and **Multipart upload for faster writes**
  - use parallel threads and **Range Header GET for faster reads**
  - for list operations with large number of objects, its better to build a secondary index in DynamoDB
  - use **Versioning to protect from unintended overwrites and deletions**, but this does not protect against bucket deletion
  - use **VPC S3 Endpoints** with VPC to transfer data using Amazon internet network

## Use scenarios

- **Backup and Storage** - Provide data backup and storage services for others.
- **Application Hosting** - Provide services that deploy, install, and manage web applications.
- **Media Hosting** - Build a redundant, scalable, and highly available infrastructure that hosts video, photo, or music uploads and downloads.
- **Software Delivery** - Host your software applications that customers can download.

> [Service overview](https://aws.amazon.com/s3/)
